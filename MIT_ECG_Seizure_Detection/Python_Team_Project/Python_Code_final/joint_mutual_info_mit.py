# -*- coding: utf-8 -*-
"""Joint_Mutual_Info_MIT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G_oG2dbGXTJT1RTVllXp2mg_Fnu2-B_u
"""

#Designed by Muhammad Farhan Azmine
!pip install git+https://github.com/danielhomola/mifs

import mifs


dataFrame = pd.read_excel('corrected_group_MIT1.xlsx')

# load X and y
# define MI_FS feature selection method

y=dataFrame.target
#print(y)
X=dataFrame.drop(["target"],axis=1)

from numpy.core.fromnumeric import shape
from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from numpy.lib.function_base import average 
from re import U 
from sklearn import tree 
from sklearn.model_selection import train_test_split 
import pydot 
import graphviz 
import pandas as pd 
import pydotplus 
import collections
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDClassifier

from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.preprocessing import MinMaxScaler
import numpy as np

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import validation_curve


from sklearn.preprocessing import LabelEncoder, OneHotEncoder

from sklearn.compose import ColumnTransformer



from sklearn.linear_model import LogisticRegression
from sklearn import linear_model as linmod
from sklearn import metrics
from sklearn import preprocessing as preproc
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV
from sklearn.metrics import accuracy_score







n_features=50

#feat_selector = mifs.MutualInformationFeatureSelector(method="JMI",verbose=1,n_features=14)
feat_selector = mifs.MutualInformationFeatureSelector(method="JMI",verbose=1,n_features=n_features)

# find all relevant features
feat_selector.fit(X, y)

import gc

gc.disable()
print(X)
# check selected features
print(feat_selector._support_mask)

# check ranking of features
print(feat_selector.ranking_)

# call transform() on X to filter it down to selected features
X_filtered = feat_selector.transform(X)

X_filtered=pd.DataFrame(X_filtered)

print(X_filtered)

X_filtered.to_csv("X_filtered_50.csv")

from sklearn.model_selection import cross_val_score
import numpy as np

xtrain, xtest, ytrain, ytest = train_test_split(X_filtered, y, test_size=0.3, random_state=42)
model = linmod.LogisticRegression(max_iter=10000,tol=1e-6,solver='saga') 
model.fit(xtrain, ytrain)
Ypred=model.predict(xtest)
print ("Classification accuracy = %f" % metrics.accuracy_score(ytest, Ypred))
print("Cross validation scores")
cross_scores=cross_val_score(model,X_filtered,y)
print(np.max(cross_scores))
print("number of features")
print(n_features)