# -*- coding: utf-8 -*-
"""Hjorth_coeffs_on_vt_gmail.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13NIjtVe1EVF48zxQtX3Ki9-Ng0dvPbZZ
"""

!pip install git+https://github.com/forrestbao/pyeeg

!pip install pandas

!pip install glob

#Code designed by Muhammad Farhan Azmine

from numpy.lib.function_base import average 
from re import U 
from sklearn import tree 
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split 
import pydot 
import graphviz 

import pydotplus 
import collections
import matplotlib.pyplot as plt

import pyeeg

import pandas as pd


from sklearn.linear_model import SGDClassifier


from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

import numpy as np
from sklearn.datasets import make_regression 
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import validation_curve

from scipy.stats import zscore

from glob import glob

import pandas as pd


path="/content/drive/MyDrive/Real_time data/MIT_real/NS_Segs-20221207T015102Z-001.zip/*.csv"
all_files = glob(path)
all_files=sorted(all_files)
data=[]


#print(all_files)

for filename in all_files:

  #df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7','T7-P7','P7-O1',
	  #                              	'FP1-F3',	'F3-C3',	'C3-P3',	'P3-O1'	,'FP2-FP4','F4-C4','C4-P4','P4-02','FP2-F8','F8-T8','T8-P8','P8-O2','FZ-CZ','CZ-PZ','P7-T7','T7-FT9','FT9-FT10','FT10-T8','T8-P8-2'])
  
  
  
  df=pd.read_csv(filename)
  df=df.rename(columns=str.upper)
  #df=df[['fp1',	'f3',	'c3',	'p3',	'o1',	'f7',	't3',	't5',	'fc1',	'fc5', 'cp1',	'cp5',	'f9',	'fz',	'cz',	'pz',	'fp2',	'f4',	'c4',	'p4',	'o2',	'f8',	't4',	't6',	'fc2',	'fc6',	'cp2',	'cp6',	'f10']].copy()
  
 # df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7'])
  #**df=df[['FP1-F7', 'F7-T7','T7-P7','P7-O1',
	   #                             	'FP1-F3',	'F3-C3',	'C3-P3',	'P3-O1'	,'FP2-F4','F4-C4','C4-P4','P4-O2','FP2-F8','F8-T8','T8-P8','P8-O2','FZ-CZ','CZ-PZ']]
  
  df=df[['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3','F3-C3','C3-P3','P3-O1','FP2-F4','F4-C4', 'C4-P4', 'P4-O2',
                                         'FP2-F8','F8-T8','T8-P8','P8-O2','FZ-CZ','CZ-PZ']]
 
  #print(type(df))
  
  
  data.append(df)
  
  #print(data)
  
  

df=pd.concat(data,axis=0,ignore_index=True)
#print(df)
#data=pd.concat(df,ignore_index=True)

#df=pd.concat(data,axis=0,ignore_index=True)




def get_features(sig,variance,m):
    #print(signal)
                                                                         
    # freq_cutoffs = [3, 8, 12, 27, 40, 59, 61, 80, 100, 120]

    features = []
    print(type(sig))


    #print(sig)
    #print(type(sig))

    # features.append(rms(sig))

    # s = lpf(sig, freq_cutoffs[0], sampling_rate)
    # features.append(rms(s))

    # for i in range(len(freq_cutoffs)-1):
    #     s = bp(sig, freq_cutoffs[i], freq_cutoffs[i+1], sampling_rate)
    #     features.append(rms(s))

    # fourier = np.fft.rfft(sig * np.hamming(sig.size))
    # features.extend(abs(fourier))

    # features.append(pyeeg.hurst(sig))
    # features.append(pyeeg.hfd(sig, 10))
    # e = pyeeg.spectral_entropy(sig, np.append(0.5, freq_cutoffs), sampling_rate)
    # features.append(e)
    
    # very fast, but slow overall =
    #for i in range(4)
       
    #print(sig.iloc[:,1])
    #for i in range
    
    for i in range(18):
      
      a=np.ravel(sig.iloc[:,i])

      features.append(pyeeg.hjorth(a))
  # features.append(pyeeg.pfd(sig))
    #print(features)
    #features.append(np.mean(sig))
    #features.append(np.std(sig))

    #features.append(scipy.stats.skew(sig))
    #features.append(scipy.stats.kurtosis(sig))


    ##features.extend(sig)
    #features=np.ravel(features)
    
    variance=variance.to_numpy()
    #features=np.ravel(features)
    #features.append(variance)
    features=pd.DataFrame(features)
    print(features)
    df=pd.DataFrame({'mobility of %s'%(all_files[m]):features[0],'complexity of %s'%(all_files[m]):features[1], 'variance of %s'%(all_files[m]) :variance})
    
    

    print(df)
    #*features.to_csv("file_name.csv", index=True)
    #df2=pd.merge(df.iloc[0,:],df.iloc[1,:])
    
    
    return df

m=0
k=0
r=[]
r=pd.DataFrame(r)

for j in range(0,len(all_files),1):
  
  dataFrame=(df.iloc[k:k+1280]) 
  k=k+1281
  #**print(dataFrame)
  #*dataFrame= pd.read_excel(fileName)

  #print(dataFrame)
  
  x=dataFrame
  #scalerX = StandardScaler() 
  #scalerX.fit(x) 
  #x = zscore(x)


  #print(type(dataFrame))
  #**print(type(x))
 


  variance = x.var()

  #**print(type(variance))

  #x = zscore(x)








  segments=get_features(x,variance,m)
  m=m+1
  print(type(segments))
  #segments=segments.to_numpy()
  #segments=pd.DataFrame(segments,index=None)
  #segments=pd.DataFrame(segments)
  r=pd.concat([r,segments],axis=1)
  #r.append(segments,ignore_index=True)
   
  
print(type(r)) 



r.to_csv("Real_time_MIT_revised_dataset_Only_Seizure_segs.csv", index=True)

print(all_files)

dframe=r
print(dframe)

from numpy.lib.function_base import average 
from re import U 
from sklearn import tree 
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split 
import pydot 
import graphviz 

import pydotplus 
import collections
import matplotlib.pyplot as plt

import pyeeg

import pandas as pd


from sklearn.linear_model import SGDClassifier


from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

import numpy as np
from sklearn.datasets import make_regression 
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import validation_curve

from scipy.stats import zscore
import csv
from glob import glob


#path="/content/drive/MyDrive/Final MIT dataset/MIT/NS_Segs/*.csv"

path="/content/drive/MyDrive/RT_Final_Siena/NS_Segs/*.csv"

all_files = glob(path)
#all_files=sorted(all_files)
data=[]

print(all_files)

for filename in all_files:

  #df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7','T7-P7','P7-O1',
	  #                              	'FP1-F3',	'F3-C3',	'C3-P3',	'P3-O1'	,'FP2-FP4','F4-C4','C4-P4','P4-02','FP2-F8','F8-T8','T8-P8','P8-O2','FZ-CZ','CZ-PZ','P7-T7','T7-FT9','FT9-FT10','FT10-T8','T8-P8-2'])
  
  
  
  df=pd.read_csv(filename)
  df=df.rename(columns=str.upper)
  df=df[['CP1',	'CP5']]
  
 # df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7'])
  #df=df[['FP1-F7', 'F7-T7','T7-P7','P7-O1',
	 #                               	'FP1-F3',	'F3-C3',	'C3-P3',	'P3-O1'	,'FP2-FP4','F4-C4','C4-P4','P4-02','FP2-F8','F8-T8','T8-P8','P8-O2','FZ-CZ','CZ-PZ','P7-T7','T7-FT9','FT9-FT10','FT10-T8','T8-P8-2']].copy
  data.append(df)
  

df=pd.concat(data,axis=0,ignore_index=True)



print(df)

df.to_csv("Rest_2_data.csv", index=True)

df=pd.read_excel("all_data.xlsx")

df=df[['FP1',	'F3',	'C3',	'P3',	'O1',	'F7',	'T3',	'T5',	'FC1',	'FC5', 'CP1',	'CP5',	'F9',	'FZ',	'CZ',	'PZ',	'FP2',	'F4',	'C4',	'P4',	'O2',	'F8',	'T4',	'T6',	'FC2',	'FC6',	'CP2',	'CP6',	'F10']]
  

def get_features(sig,variance,m):
    #print(signal)
                                                                         
    # freq_cutoffs = [3, 8, 12, 27, 40, 59, 61, 80, 100, 120]

    features = []
    print(type(sig))


    #print(sig)
    #print(type(sig))

    # features.append(rms(sig))

    # s = lpf(sig, freq_cutoffs[0], sampling_rate)
    # features.append(rms(s))

    # for i in range(len(freq_cutoffs)-1):
    #     s = bp(sig, freq_cutoffs[i], freq_cutoffs[i+1], sampling_rate)
    #     features.append(rms(s))

    # fourier = np.fft.rfft(sig * np.hamming(sig.size))
    # features.extend(abs(fourier))

    # features.append(pyeeg.hurst(sig))
    # features.append(pyeeg.hfd(sig, 10))
    # e = pyeeg.spectral_entropy(sig, np.append(0.5, freq_cutoffs), sampling_rate)
    # features.append(e)
    
    # very fast, but slow overall =
    #for i in range(4)
       
    #print(sig.iloc[:,1])
    #for i in range
    
    for i in range(29):
      
      a=np.ravel(sig.iloc[:,i])

      features.append(pyeeg.hjorth(a))
  # features.append(pyeeg.pfd(sig))
    #print(features)
    #features.append(np.mean(sig))
    #features.append(np.std(sig))

    #features.append(scipy.stats.skew(sig))
    #features.append(scipy.stats.kurtosis(sig))


    ##features.extend(sig)
    #features=np.ravel(features)
    
    variance=variance.to_numpy()
    #features=np.ravel(features)
    #features.append(variance)
    features=pd.DataFrame(features)
    print(features)
    df=pd.DataFrame({'mobility of %s'%(all_files[m]):features[0],'complexity of %s'%(all_files[m]):features[1], 'variance of %s'%(all_files[m]) :variance})
    
    

    print(df)
    #*features.to_csv("file_name.csv", index=True)
    #df2=pd.merge(df.iloc[0,:],df.iloc[1,:])
    
    
    return df

m=0
k=0
r=[]
r=pd.DataFrame(r)

for j in range(0,len(all_files),1):
  
  dataFrame=(df.iloc[k:k+2560]) 
  k=k+2561
  #**print(dataFrame)
  #*dataFrame= pd.read_excel(fileName)

  #print(dataFrame)
  
  x=dataFrame
  #scalerX = StandardScaler() 
  #scalerX.fit(x) 
  #x = zscore(x)


  #print(type(dataFrame))
  #**print(type(x))



  variance = x.var()

  #**print(type(variance))









  segments=get_features(x,variance,m)
  m=m+1
  print(type(segments))
  #segments=segments.to_numpy()
  #segments=pd.DataFrame(segments,index=None)
  #segments=pd.DataFrame(segments)
  r=pd.concat([r,segments],axis=1)
  #r.append(segments,ignore_index=True)
   
  
print(r) 

r.to_csv("Real_time_Hjorth_for_Non_Seizure_Siena_types.csv", index=True)

from numpy.lib.function_base import average 
from re import U 
from sklearn import tree 
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split 
import pydot 
import graphviz 

import pydotplus 
import collections
import matplotlib.pyplot as plt

import pyeeg

import pandas as pd


from sklearn.linear_model import SGDClassifier


from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

import numpy as np
from sklearn.datasets import make_regression 
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import validation_curve

from scipy.stats import zscore
import csv
from glob import glob

path="/content/drive/MyDrive/Ns_segs_for_project/Data_for_siena/NS_Segs/*.csv"
all_files = glob(path)
all_files=sorted(all_files)
data=[]

print(all_files[0])
df=pd.read_csv(all_files[0])
print(df)

for filename in all_files:

  #df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7','T7-P7','P7-O1',
	 #                               	'FP1-F3',	'F3-C3',	'C3-P3',	'P3-O1'	,'FZ-CZ',	'CZ-PZ',	'FP2-F4',	'F4-C4',	'C4-P4',	'P4-O2',	'FP2-F8',	'F8-T8',	'T8-P8',	'P8-O2'])
  df=pd.read_csv(filename)
  df=df.rename(columns=str.lower)
  df=df[['fp1',	'f3',	'c3',	'p3',	'o1',	'f7',	't3',	't5',	'fc1',	'fc5', 'cp1',	'cp5',	'f9',	'fz',	'cz',	'pz',	'fp2',	'f4',	'c4',	'p4',	'o2',	'f8',	't4',	't6',	'fc2',	'fc6',	'cp2',	'cp6',	'f10']].copy()
  
 # df=pd.read_csv(filename,usecols=['FP1-F7', 'F7-T7'])
  data.append(df)
  

df=pd.concat(data,axis=0,ignore_index=True)



#print(df)